{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#W207 Machine Learning Final Project #\n",
    "###Forest Cover Type Prediction###\n",
    "Amitava Das & Katherine Shelley"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Data Dictionary###"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Name                                     Data Type    Measurement                       Description\n",
    "\n",
    "Elevation                               quantitative    meters                       Elevation in meters\n",
    "Aspect                                  quantitative    azimuth                      Aspect in degrees azimuth\n",
    "Slope                                   quantitative    degrees                      Slope in degrees\n",
    "Horizontal_Distance_To_Hydrology        quantitative    meters                       Horz Dist to nearest surface water features\n",
    "Vertical_Distance_To_Hydrology          quantitative    meters                       Vert Dist to nearest surface water features\n",
    "Horizontal_Distance_To_Roadways         quantitative    meters                       Horz Dist to nearest roadway\n",
    "Hillshade_9am                           quantitative    0 to 255 index               Hillshade index at 9am, summer solstice\n",
    "Hillshade_Noon                          quantitative    0 to 255 index               Hillshade index at noon, summer soltice\n",
    "Hillshade_3pm                           quantitative    0 to 255 index               Hillshade index at 3pm, summer solstice\n",
    "Horizontal_Distance_To_Fire_Points      quantitative    meters                       Horz Dist to nearest wildfire ignition points\n",
    "Wilderness_Area (4 binary columns)      qualitative     0 (absence) or 1 (presence)  Wilderness area designation\n",
    "Soil_Type (40 binary columns)           qualitative     0 (absence) or 1 (presence)  Soil Type designation\n",
    "Cover_Type (7 types)                    integer         1 to 7                       Forest Cover Type designation\n",
    "\n",
    "\n",
    "Code Designations:\n",
    "\n",
    "Wilderness Areas:  \t1 -- Rawah Wilderness Area\n",
    "                    2 -- Neota Wilderness Area\n",
    "                    3 -- Comanche Peak Wilderness Area\n",
    "                    4 -- Cache la Poudre Wilderness Area\n",
    "\n",
    "Soil Types:             1 to 40 : based on the USFS Ecological\n",
    "                        Landtype Units (ELUs) for this study area:\n",
    "\n",
    "  Study Code USFS ELU Code\t\t\tDescription\n",
    "\t 1\t   2702\t\tCathedral family - Rock outcrop complex, extremely stony.\n",
    "\t 2\t   2703\t\tVanet - Ratake families complex, very stony.\n",
    "\t 3\t   2704\t\tHaploborolis - Rock outcrop complex, rubbly.\n",
    "\t 4\t   2705\t\tRatake family - Rock outcrop complex, rubbly.\n",
    "\t 5\t   2706\t\tVanet family - Rock outcrop complex complex, rubbly.\n",
    "\t 6\t   2717\t\tVanet - Wetmore families - Rock outcrop complex, stony.\n",
    "\t 7\t   3501\t\tGothic family.\n",
    "\t 8\t   3502\t\tSupervisor - Limber families complex.\n",
    "\t 9\t   4201\t\tTroutville family, very stony.\n",
    "\t10\t   4703\t\tBullwark - Catamount families - Rock outcrop complex, rubbly.\n",
    "\t11\t   4704\t\tBullwark - Catamount families - Rock land complex, rubbly.\n",
    "\t12\t   4744\t\tLegault family - Rock land complex, stony.\n",
    "\t13\t   4758\t\tCatamount family - Rock land - Bullwark family complex, rubbly.\n",
    "\t14\t   5101\t\tPachic Argiborolis - Aquolis complex.\n",
    "\t15\t   5151\t\tunspecified in the USFS Soil and ELU Survey.\n",
    "\t16\t   6101\t\tCryaquolis - Cryoborolis complex.\n",
    "\t17\t   6102\t\tGateview family - Cryaquolis complex.\n",
    "\t18\t   6731\t\tRogert family, very stony.\n",
    "\t19\t   7101\t\tTypic Cryaquolis - Borohemists complex.\n",
    "\t20\t   7102\t\tTypic Cryaquepts - Typic Cryaquolls complex.\n",
    "\t21\t   7103\t\tTypic Cryaquolls - Leighcan family, till substratum complex.\n",
    "\t22\t   7201\t\tLeighcan family, till substratum, extremely bouldery.\n",
    "\t23\t   7202\t\tLeighcan family, till substratum - Typic Cryaquolls complex.\n",
    "\t24\t   7700\t\tLeighcan family, extremely stony.\n",
    "\t25\t   7701\t\tLeighcan family, warm, extremely stony.\n",
    "\t26\t   7702\t\tGranile - Catamount families complex, very stony.\n",
    "\t27\t   7709\t\tLeighcan family, warm - Rock outcrop complex, extremely stony.\n",
    "\t28\t   7710\t\tLeighcan family - Rock outcrop complex, extremely stony.\n",
    "\t29\t   7745\t\tComo - Legault families complex, extremely stony.\n",
    "\t30\t   7746\t\tComo family - Rock land - Legault family complex, extremely stony.\n",
    "\t31\t   7755\t\tLeighcan - Catamount families complex, extremely stony.\n",
    "\t32\t   7756\t\tCatamount family - Rock outcrop - Leighcan family complex, extremely stony.\n",
    "\t33\t   7757\t\tLeighcan - Catamount families - Rock outcrop complex, extremely stony.\n",
    "\t34\t   7790\t\tCryorthents - Rock land complex, extremely stony.\n",
    "\t35\t   8703\t\tCryumbrepts - Rock outcrop - Cryaquepts complex.\n",
    "\t36\t   8707\t\tBross family - Rock land - Cryumbrepts complex, extremely stony.\n",
    "\t37\t   8708\t\tRock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.\n",
    "\t38\t   8771\t\tLeighcan - Moran families - Cryaquolls complex, extremely stony.\n",
    "\t39\t   8772\t\tMoran family - Cryorthents - Leighcan family complex, extremely stony.\n",
    "\t40\t   8776\t\tMoran family - Cryorthents - Rock land complex, extremely stony.\n",
    "\n",
    "        Note:   First digit:  climatic zone             Second digit:  geologic zones\n",
    "                1.  lower montane dry                   1.  alluvium\n",
    "                2.  lower montane                       2.  glacial\n",
    "                3.  montane dry                         3.  shale\n",
    "                4.  montane                             4.  sandstone\n",
    "                5.  montane dry and montane             5.  mixed sedimentary\n",
    "                6.  montane and subalpine               6.  unspecified in the USFS ELU Survey\n",
    "                7.  subalpine                           7.  igneous and metamorphic\n",
    "                8.  alpine                              8.  volcanic\n",
    "\n",
    "        The third and fourth ELU digits are unique to the mapping unit \n",
    "        and have no special meaning to the climatic or geologic zones.\n",
    "\n",
    "Forest Cover Type Classes:\t1 -- Spruce/Fir\n",
    "                            2 -- Lodgepole Pine\n",
    "                            3 -- Ponderosa Pine\n",
    "                            4 -- Cottonwood/Willow\n",
    "                            5 -- Aspen\n",
    "                            6 -- Douglas-fir\n",
    "                            7 -- Krummholz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "#libraries for cleaning/feature selection\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "#classifiers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.mixture import GMM\n",
    "\n",
    "#metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part 1: Data Collection and Baseline Estimation##\n",
    "First we will download the data and split it into a training, development, and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Load the data###\n",
    "First let's load the data into a Pandas dataframe. The data has a lot of dummy binary features, which means there will be a lot of entries that are zero and we'll be adding more binary features to the data set. We will use a sparse representation in both Pandas and Numpy to save space and reduce computation time. Read more about Pandas sparse representation here: http://pandas.pydata.org/pandas-docs/stable/sparse.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.sparse.frame.SparseDataFrame'>\n",
      "(15120, 54)\n",
      "<class 'pandas.sparse.frame.SparseDataFrame'>\n",
      "(565892, 54)\n"
     ]
    }
   ],
   "source": [
    "#read the training data into a pandas dataframe\n",
    "data = os.path.join(os.getcwd(),'train.csv')\n",
    "df_train = pd.read_csv(data).to_sparse() \n",
    "#separate out the training labels, drop the Id column from the training dataset\n",
    "Y = np.array(df_train['Cover_Type']) #extract the labels\n",
    "df_train = df_train.drop(['Id','Cover_Type'],1)\n",
    "X = df_train.as_matrix()\n",
    "\n",
    "#read the test data into a pandas frame\n",
    "data = os.path.join(os.getcwd(),'test.csv')\n",
    "df_test = pd.read_csv(data).to_sparse()\n",
    "#separate out the Id column, keep it for Kaggle submissions\n",
    "test_ids = np.array(df_test['Id'])\n",
    "df_test = df_test.drop(['Id'],1)\n",
    "test_data = df_test.as_matrix()\n",
    "\n",
    "#You can see we have loaded the data into sparse dataframes\n",
    "print type(df_train)\n",
    "print df_train.shape\n",
    "print type(df_test)\n",
    "print df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Baseline model###\n",
    "We initially created a baseline model using the K-Nearest Neighbors classifier with n_neighbors set to 1. With this baseline we were able to obtain an accuracy of 71.06% against the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved  565892 records to BaseSubmission.csv\n"
     ]
    }
   ],
   "source": [
    "def InitialBaseline(X,Y,test_data,test_ids):\n",
    "    # Instantiate the KNeighborsClassifier class with appropriate k for entire data set\n",
    "    clf = KNeighborsClassifier(n_neighbors=1)\n",
    "    clf.fit(X,Y)\n",
    "    predicted = clf.predict(test_data)\n",
    "    outfile = zip(test_ids, predicted)\n",
    "    np.savetxt(\"BaseSubmission.csv\", outfile, fmt='%i', delimiter=',', newline='\\n', header='Id, Cover_Type',comments='')\n",
    "    print \"Saved \", len(outfile),\"records to BaseSubmission.csv\"\n",
    "\n",
    "InitialBaseline(X,Y,test_data,test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part 2: Exploring Different Models##\n",
    "Next we will use Grid Search to analyze various models to see if we get improvement simply by switching from Nearest Neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (10080, 54)\n",
      "Development data: (5040, 54)\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the inputs X and Y.\n",
    "# NOTE: Each time you run this cell, you'll re-shuffle the data, resulting in a different ordering.\n",
    "X, Y = shuffle(X,Y,random_state=0) #shuffle the sparse matrix and dense array\n",
    "\n",
    "# Set some variables to hold dev and training data;\n",
    "N = X.shape[0]\n",
    "#test_data, test_labels = X[2*N/3:], Y[2*N/3:]\n",
    "dev_data, dev_labels = X[2*N/3:], Y[2*N/3:]\n",
    "train_data, train_labels = X[:2*N/3], Y[:2*N/3]\n",
    "\n",
    "print \"Training data: %s\" % str(train_data.shape)\n",
    "print \"Development data: %s\" % str(dev_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###Gaussian Mixture Model###\n",
    "Perhaps a clustering model will give us better results. Since we know we have 7 cover types we will test various covariance types for a 7 component mixture model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greatest Accuracy: 0.508333333333 Using 3 PCA components, 1 Gaussians, with diag Covariance Type.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def components(p,g,type_str):\n",
    "    #returns number of GMM components given features, gaussians, covariance type\n",
    "    if type_str == 'full': \n",
    "        return (2*p + p*(p-1)/2)*g\n",
    "    elif type_str == 'diag': \n",
    "        return (2*p)*g\n",
    "    elif type_str == 'spherical': \n",
    "        return (p+1)*g\n",
    "    elif type_str == 'tied': \n",
    "        return p*g + (p+ p*(p-1)/2)\n",
    "    else: \n",
    "        return 9999\n",
    "    \n",
    "def get_n_components(n,classes=2):\n",
    "    #returns a list of tuples representing PCA/GMM parameters\n",
    "    #with less than n components\n",
    "    params = []\n",
    "    for i in range(1,n+1):\n",
    "        for j in range(1,n+1):\n",
    "            for cov_type in ['spherical','diag','tied','full']:\n",
    "                #if the total components is less than or equal to 50\n",
    "                if classes*components(i,j,cov_type) <= n:\n",
    "                    params.append((i,j,cov_type))\n",
    "    return params\n",
    "\n",
    "def GMM_test():\n",
    "    #Different component sizes for different covariance matrices:\n",
    "    #Full: n means, n variances, and n(n-1)/2 covariances (symmetric)  per Gaussian\n",
    "    #Tied: n means per Gaussian, n variances, and n(n-1)/2 covariances for all Gaussians \n",
    "    #Spherical: n means, 1 variance = n+1 per Gaussian\n",
    "    #Diagonal: n means, n variances = 2*n per Gaussian\n",
    "    best_pca,best_components,best_cov,best_accuracy = 0,0,'',0 #initialize the best parameters\n",
    "    for i,j,cov_type in get_n_components(50,7):\n",
    "        #initiate the PCA model, fit to training data, transform both train and dev data\n",
    "        pca = PCA(n_components = i)\n",
    "        reduced_train = pca.fit_transform(train_data)\n",
    "        reduced_dev = pca.transform(dev_data)\n",
    "        #split out the training examples\n",
    "        train1 = reduced_train[np.where(train_labels==1)[0]] #type 1\n",
    "        train2 = reduced_train[np.where(train_labels==2)[0]] #type 2\n",
    "        train3 = reduced_train[np.where(train_labels==3)[0]] #type 3\n",
    "        train4 = reduced_train[np.where(train_labels==4)[0]] #type 4\n",
    "        train5 = reduced_train[np.where(train_labels==5)[0]] #type 5\n",
    "        train6 = reduced_train[np.where(train_labels==6)[0]] #type 6\n",
    "        train7 = reduced_train[np.where(train_labels==7)[0]] #type 7\n",
    "\n",
    "        #initiate and fit a GMM for the type 1 reduced data\n",
    "        gmm1 = GMM(n_components=j,covariance_type=cov_type)\n",
    "        gmm1.fit(train1)\n",
    "        pred1 = np.exp(gmm1.score(reduced_dev)) #predict the probability of positive for the reduced test data\n",
    "        \n",
    "        #initiate and fit a GMM for the type 2 reduced data\n",
    "        gmm2 = GMM(n_components=j,covariance_type=cov_type)\n",
    "        gmm2.fit(train2)\n",
    "        pred2 = np.exp(gmm2.score(reduced_dev)) #predict the probability of positive for the reduced test data\n",
    "        \n",
    "        #initiate and fit a GMM for the type 3 reduced data\n",
    "        gmm3 = GMM(n_components=j,covariance_type=cov_type)\n",
    "        gmm3.fit(train3)\n",
    "        pred3 = np.exp(gmm3.score(reduced_dev)) #predict the probability of positive for the reduced test data\n",
    "        \n",
    "        #initiate and fit a GMM for the type 4 reduced data\n",
    "        gmm4 = GMM(n_components=j,covariance_type=cov_type)\n",
    "        gmm4.fit(train4)\n",
    "        pred4 = np.exp(gmm4.score(reduced_dev)) #predict the probability of positive for the reduced test data\n",
    "        \n",
    "        #initiate and fit a GMM for the type 5 reduced data\n",
    "        gmm5 = GMM(n_components=j,covariance_type=cov_type)\n",
    "        gmm5.fit(train5)\n",
    "        pred5 = np.exp(gmm5.score(reduced_dev)) #predict the probability of positive for the reduced test data\n",
    "        \n",
    "        #initiate and fit a GMM for the type 6 reduced data\n",
    "        gmm6 = GMM(n_components=j,covariance_type=cov_type)\n",
    "        gmm6.fit(train6)\n",
    "        pred6 = np.exp(gmm6.score(reduced_dev)) #predict the probability of positive for the reduced test data\n",
    "        \n",
    "        #initiate and fit a GMM for the type 7 reduced data\n",
    "        gmm7 = GMM(n_components=j,covariance_type=cov_type)\n",
    "        gmm7.fit(train7)\n",
    "        pred7 = np.exp(gmm7.score(reduced_dev)) #predict the probability of negative for the reduced test data\n",
    "        \n",
    "        probs = np.max(np.array([pred1,pred2,pred3,pred4,pred5,pred6,pred7]),axis=0) #get the max probabilities\n",
    "        \n",
    "        prediction = np.zeros(dev_labels.shape) #set a vector of zeros for predictions\n",
    "        \n",
    "        prediction[np.where(pred1 == probs)[0]]=1 #predicted 1\n",
    "        prediction[np.where(pred2 == probs)[0]]=2 #predicted 2\n",
    "        prediction[np.where(pred3 == probs)[0]]=3 #predicted 3\n",
    "        prediction[np.where(pred4 == probs)[0]]=4 #predicted 4\n",
    "        prediction[np.where(pred5 == probs)[0]]=5 #predicted 5\n",
    "        prediction[np.where(pred6 == probs)[0]]=6 #predicted 6\n",
    "        prediction[np.where(pred7 == probs)[0]]=7 #predicted 7\n",
    "\n",
    "        accuracy = np.mean(prediction==dev_labels)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_pca = i\n",
    "            best_components = j\n",
    "            best_cov = cov_type\n",
    "            best_accuracy = accuracy\n",
    "    print \"Greatest Accuracy: %s Using %s PCA components, %s Gaussians, with %s Covariance Type.\" % (best_accuracy,best_pca,best_components,best_cov)\n",
    "GMM_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###Logistic Regression###\n",
    "Gaussian Mixture Models did not give us very good accuracy. Let's see if we can do better with Logistic Regression. Logistic Regression is a popular choice for classification because "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.6388888889 Accuracy using C = 10000 and Penalty Function l1\n"
     ]
    }
   ],
   "source": [
    "lm = LogisticRegression() #initialize a logistic regression model\n",
    "parameters = {'C':[10**i for i in range(-5,5)],'penalty':['l1','l2']}\n",
    "clf = GridSearchCV(lm, parameters)\n",
    "clf.fit(train_data,train_labels)\n",
    "print '%s Accuracy using C = %s and Penalty Function %s' % (clf.score(dev_data,dev_labels)*100,clf.best_params_['C'],clf.best_params_['penalty'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Random Forests###\n",
    "Now Let's test Random Forests for various values of n trees. Note, this cell can take several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest gives 86.0714285714 percent Accuracy using n = 1000 trees and gini criterion\n"
     ]
    }
   ],
   "source": [
    "parameters = {'n_estimators':[10**i for i in range(4)],'criterion':['gini','entropy']}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "clf = GridSearchCV(rf, parameters)\n",
    "clf.fit(train_data,train_labels)\n",
    "print 'Random Forest gives %s percent Accuracy using n = %s trees and %s criterion' % (clf.score(dev_data,dev_labels)*100,clf.best_params_['n_estimators'],clf.best_params_['criterion'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests have given by far the best accuracy that we have seen. This is not surprising considering the amount of binary features in our data set. From here on we will use this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part 3: Improving Our Model##\n",
    "Next we will explore feature engineering and dimensionality reduction to try to improve the accuracy of our Random Forest classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Feature Engineering###\n",
    "Next we will perform feature engineering:\n",
    "1. Create new variables that represent the climatic and geologic soil zones (soil id prefix values)\n",
    "2. Convert degrees fields to radians.\n",
    "3. Add new binary variables to represent degree field \"quadrants\", i.e. 0 - 90 degrees, 90 to 180 degrees, etc. (in radians)\n",
    "4. Normalize hillshade features from 0 to 255 to 0 to 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15120, 69)\n"
     ]
    }
   ],
   "source": [
    "#Functions for feature engineering\n",
    "def add_soil_prefix_fields(df):\n",
    "    #adds new variables to a dataframe that represent the climatic and geologic zones \n",
    "    #(first two digits of the soil type variable)\n",
    "    #we'll use a dicitonary to isolate the soil id's by their climatic/geologic prefix\n",
    "    soil_id = {'27':['1','2','3','4','5','6'],\n",
    "               '35':['7','8'],\n",
    "               '42':['9'],\n",
    "               '47':['10','11','12','13'],\n",
    "               '51':['14','15'],\n",
    "               '61':['16','17'],\n",
    "               '67':['18'],\n",
    "               '71':['19','20','21'],\n",
    "               '72':['22','23'],\n",
    "               '77':['24','25','26','27','28','29','30','31','32','33','34'],\n",
    "               '87':['35','36','37','38','39','40']\n",
    "              }\n",
    "\n",
    "    climatic = {'2':['1','2','3','4','5','6'],\n",
    "                '3':['7','8'],\n",
    "               '4':['9','10','11','12','13'],\n",
    "               '5':['14','15'],\n",
    "               '6':['16','17','18'],\n",
    "               '7':['19','20','21','22','23','24','25','26','27','28','29','30','31','32','33','34'],\n",
    "               '8':['35','36','37','38','39','40']\n",
    "               }\n",
    "\n",
    "    geologic = {'1':['14','15','16','17','19','20','21'],\n",
    "                '2':['9','22','23'],\n",
    "                '5':['7','8'],\n",
    "                '7':['1','2','3','4','5','6','10','11','12','13','18','24','25','26','27','28',\n",
    "                     '29','30','31','32','33','34','35','36','37','38','39','40']\n",
    "               }\n",
    "\n",
    "    #add new variables for the soil type prefix, climatic zone, and geologic zone\n",
    "    for key in soil_id:\n",
    "        clim,geol = key[0],key[1]\n",
    "        df_clim = df[['Soil_Type%s' % i for i in climatic[clim]]] #subset of df of climatic types\n",
    "        df_geol = df[['Soil_Type%s' % i for i in geologic[geol]]] #subset of df of climatic types\n",
    "\n",
    "        df['climatic zone %s' % clim] = np.sum(np.asarray(df_clim),axis=1)\n",
    "        df['geologic zone %s' % geol] = np.sum(np.asarray(df_geol),axis=1)\n",
    "        del df_clim\n",
    "        del df_geol\n",
    "    return df\n",
    "\n",
    "def degrees_to_radians(df):\n",
    "    for col in ['Aspect','Slope']:\n",
    "        df[col] = np.radians(df[col].as_matrix())\n",
    "    return df\n",
    "\n",
    "def add_aspect_quadrants(df):\n",
    "    df['Aspect0_90'] = np.where(df['Aspect']<0.5,np.ones(df.shape[0]),np.zeros(df.shape[0]))\n",
    "    df['Aspect90_180'] = np.where(df['Aspect']<1,np.ones(df.shape[0]),np.zeros(df.shape[0])) - np.array(df['Aspect0_90'])\n",
    "    df['Aspect180_270'] = np.where(df['Aspect']<1.5,np.ones(df.shape[0]),np.zeros(df.shape[0])) - np.array(df['Aspect0_90'] + df['Aspect90_180'])\n",
    "    df['Aspect270_360'] = np.where(df['Aspect']>=1.5,np.ones(df.shape[0]),np.zeros(df.shape[0]))\n",
    "    return df\n",
    "\n",
    "def scale_shade_value(df):\n",
    "    for col in ['Hillshade_9am','Hillshade_Noon','Hillshade_3pm']:\n",
    "        df[col] = df[col]*(1.0/255)\n",
    "    return df\n",
    "\n",
    "#add fields for the soil type climatic and geologic zone prefixes\n",
    "df_train = scale_shade_value(add_aspect_quadrants(degrees_to_radians(add_soil_prefix_fields(df_train))))\n",
    "df_test = scale_shade_value(add_aspect_quadrants(degrees_to_radians(add_soil_prefix_fields(df_test))))\n",
    "\n",
    "print df_train.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Sparse Matrices###\n",
    "For our feature engineering we will convert the test dataframe to a SciPy sparse matrix. In particular we will use a Compressed Sparse Row matrix. Read more about Compressed Sparse Row matrices and their operations here: http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New training size: (15120, 69)\n",
      "<type 'numpy.ndarray'>\n",
      "New test size: (565892, 69)\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n"
     ]
    }
   ],
   "source": [
    "#variables to scale\n",
    "real_valued = ['Elevation','Aspect','Slope','Horizontal_Distance_To_Hydrology',\n",
    "       'Vertical_Distance_To_Hydrology','Horizontal_Distance_To_Roadways',\n",
    "       'Hillshade_9am','Hillshade_Noon','Hillshade_3pm','Horizontal_Distance_To_Fire_Points']\n",
    "\n",
    "X_real = df_train[real_valued].as_matrix()\n",
    "df_train = df_train.drop(real_valued,1)\n",
    "X = df_train.as_matrix()\n",
    "\n",
    "test_real = df_test[real_valued].as_matrix()\n",
    "df_test = df_test.drop(real_valued,1)\n",
    "test_data = csr_matrix(df_test)\n",
    "\n",
    "#scale and transform the (dense) real-valued features of the training and test data\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit_transform(X_real)\n",
    "scaler.transform(test_real)\n",
    "\n",
    "X = np.hstack([X_real,X])\n",
    "test_data = hstack([test_real,test_data])\n",
    "\n",
    "#delete data frames and temporary data arrays\n",
    "del df_train\n",
    "del df_test\n",
    "del X_real\n",
    "del test_real\n",
    "\n",
    "print \"New training size: %s\" % str(X.shape)\n",
    "print type(X)\n",
    "print \"New test size: %s\" % str(test_data.shape)\n",
    "print type(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Scale the data, convert to Numpy arrays####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (15120, 69)\n",
      "Tranining label shape:  (15120,)\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the inputs X and Y.\n",
    "# NOTE: Each time you run this cell, you'll re-shuffle the data, resulting in a different ordering.\n",
    "\n",
    "#First we will convert the training data set back to a dense array, the test data will always stay in a sparse matrix\n",
    "try:\n",
    "    X = X.toarray()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "X, Y = shuffle(X,Y,random_state=0) #shuffle the sparse matrix and dense array\n",
    "print 'Training data shape: ', X.shape\n",
    "print 'Tranining label shape: ', Y.shape\n",
    "#print 'Test data shape: ', test_data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Improving the model###\n",
    "Now that we have a baseline prediciton score, let's see what we can do to improve it. First we will separate our training data into training, development, and test sets. We will not use the test set until we have finalized our model. Tuning will be performed with the development set only to avoid overfitting to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (10080, 69)\n",
      "Development data: (5040, 69)\n"
     ]
    }
   ],
   "source": [
    "# Set some variables to hold dev and training data;\n",
    "N = X.shape[0]\n",
    "#test_data, test_labels = X[2*N/3:], Y[2*N/3:]\n",
    "dev_data, dev_labels = X[2*N/3:], Y[2*N/3:]\n",
    "train_data, train_labels = X[:2*N/3], Y[:2*N/3]\n",
    "\n",
    "print \"Training data: %s\" % str(train_data.shape)\n",
    "print \"Development data: %s\" % str(dev_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest gives 84.7222222222 percent Accuracy \n"
     ]
    }
   ],
   "source": [
    "poly = preprocessing.PolynomialFeatures(degree=2)\n",
    "poly.fit_transform(train_data)\n",
    "poly.transform(dev_data)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=1000,criterion='gini')\n",
    "rf.fit(train_data,train_labels)\n",
    "print 'Random Forest gives %s percent Accuracy ' % (rf.score(dev_data,dev_labels)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Feature Selection###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of principal components is 3\n"
     ]
    }
   ],
   "source": [
    "#let's run PCA on the data but not set any component value so we can determine how many principal components we should keep \n",
    "#to capture as much of the variance as possible.\n",
    "pca = PCA()\n",
    "pca.fit(train_data)\n",
    "\n",
    "var = 0 #retained variance\n",
    "k = 0 #number of components\n",
    "while var < 0.99:\n",
    "    k+=1\n",
    "    var = np.cumsum(pca.explained_variance_ratio_,axis=0)[k]\n",
    "print \"Optimal number of principal components is %s\" % k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest gives 87.9761904762 percent Accuracy\n"
     ]
    }
   ],
   "source": [
    "# Let's reduce the dimensionality with PCA\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "# Though maybe some original features where good, let's keep some features in their original form\n",
    "selection = SelectKBest(k=50)\n",
    "\n",
    "# Build estimator from PCA and Univariate selection\n",
    "combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n",
    "train_data_red = combined_features.fit(train_data,train_labels).transform(train_data)\n",
    "dev_data_red = combined_features.transform(dev_data)\n",
    "\n",
    "'''\n",
    "pipeline = Pipeline([(\"features\", combined_features), (\"rf\", rf)])\n",
    "\n",
    "param_grid = dict(features__pca__n_components=[3],\n",
    "                  features__univ_select__k=[1,2,3,5,10,15,20,25,40,50])\n",
    "clf = GridSearchCV(pipeline, param_grid=param_grid, verbose=10)\n",
    "clf.fit(train_data, train_labels)\n",
    "\n",
    "'''\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=1000,criterion='gini')\n",
    "rf.fit(train_data_red,train_labels)\n",
    "print 'Random Forest gives %s percent Accuracy' % (rf.score(dev_data_red,dev_labels)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python27\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:111: UserWarning: Features [16 47] are constant.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-5bc81cb23ee0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Submission3.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'%i'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Id, Cover_Type'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcomments\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Saved \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"records to Submission2.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mBestModelBaseline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-5bc81cb23ee0>\u001b[0m in \u001b[0;36mBestModelBaseline\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mcombined_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFeatureUnion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"pca\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"univ_select\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mX_red\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcombined_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mtest_data_red\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcombined_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\sklearn\\pipeline.pyc\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    473\u001b[0m         Xs = Parallel(n_jobs=self.n_jobs)(\n\u001b[0;32m    474\u001b[0m             \u001b[0mdelayed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_transform_one\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m             for name, trans in self.transformer_list)\n\u001b[0m\u001b[0;32m    476\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mXs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[0mXs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    657\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 659\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpre_dispatch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"all\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36mdispatch\u001b[1;34m(self, func, args, kwargs)\u001b[0m\n\u001b[0;32m    404\u001b[0m         \"\"\"\n\u001b[0;32m    405\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 406\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    407\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_verbosity_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, args, kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\sklearn\\pipeline.pyc\u001b[0m in \u001b[0;36m_transform_one\u001b[1;34m(transformer, name, X, transformer_weights)\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[1;31m# if we have a weight for this transformer, muliply output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtransformer_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\sklearn\\decomposition\\pca.pyc\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mean_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features)\u001b[0m\n\u001b[0;32m    332\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m         array = _ensure_sparse_format(array, accept_sparse, dtype, order,\n\u001b[1;32m--> 334\u001b[1;33m                                       copy, force_all_finite)\n\u001b[0m\u001b[0;32m    335\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.pyc\u001b[0m in \u001b[0;36m_ensure_sparse_format\u001b[1;34m(spmatrix, accept_sparse, dtype, order, copy, force_all_finite)\u001b[0m\n\u001b[0;32m    237\u001b[0m     \"\"\"\n\u001b[0;32m    238\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maccept_sparse\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m         raise TypeError('A sparse matrix was passed, but dense '\n\u001b[0m\u001b[0;32m    240\u001b[0m                         \u001b[1;34m'data is required. Use X.toarray() to '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m                         'convert to a dense numpy array.')\n",
      "\u001b[1;31mTypeError\u001b[0m: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array."
     ]
    }
   ],
   "source": [
    "def BestModelBaseline():\n",
    "    # Instantiate the KNeighborsClassifier class with appropriate k for entire data set\n",
    "    # Let's reduce the dimensionality with PCA\n",
    "    pca = PCA(n_components=3)\n",
    "\n",
    "    # Though maybe some original features where good, let's keep some features in their original form\n",
    "    selection = SelectKBest(k=50)\n",
    "\n",
    "    # Build estimator from PCA and Univariate selection\n",
    "    combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n",
    "    X_red = combined_features.fit_transform(X,Y)\n",
    "    test_data_red = combined_features.transform(test_data)\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=1000)\n",
    "    clf.fit(X_red,Y)\n",
    "    predicted = clf.predict(test_data_red)\n",
    "    outfile = zip(test_ids, predicted)\n",
    "    np.savetxt(\"Submission3.csv\", outfile, fmt='%i', delimiter=',', newline='\\n', header='Id, Cover_Type',comments='')\n",
    "    print \"Saved \", len(outfile),\"records to Submission2.csv\"\n",
    "BestModelBaseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
